dnn (深度神经网络)
cnn (卷积神经网络)
rnn (循环神经网络)

> 参考:
> - [CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？](https://www.zhihu.com/question/34681168)
> - [神经网络的每一层网络(针对特定的问题)有什么实际的意义吗？](https://www.zhihu.com/question/47732823)

## 神经网络到底需要几层?

### 先考虑一层的神经网络有什么缺点
单层的神经网络就是y=x*W+b, 他是一个线性的方程

而在现实中, 并不是全部得函数都是线性的, 比如阀值函数, 那么要拟合阀值函数就需要非线性特征.

### 为什么我们需要非线性特征

结合我们学过的线代, 与一个W矩阵相乘就是在线性变化整个空间, 在神经网络的最后一层也就是输出层(也叫全连接层)需要一个权重矩阵去实现分类, 
在分类时我们没再使用激活函数, 也就是说在最后一步是一个线性分类, 所以我们需要在输出层之前的隐含层实现数据的线性可分, 
如何将一个非线性可分的数据转换为线性可分的数据呢? 答案就是非线性变换空间

### 如何实现非线性变换

#### 1.多层的神经网络
可以参考以下文章

- [深度学习中：”多层的好处是可以用较少的参数表示复杂的函数“这句话该怎么理解？](https://www.zhihu.com/question/22473246)

总的来说就是 "神经元的数量，决定了神经网络非线性的程度。"

那么是否是层越多越好?

> 多层也有坏处，比如太多层优化起来很困难，在BP的时候会出现导数消失/爆炸的情况（vanishing/exploding gradients）

#### 2.激活函数

> 激活函数是用来加入非线性因素的

[神经网络激励函数的作用是什么？有没有形象的解释？](https://www.zhihu.com/question/22334626)


### 几层的神经网络才适合?
> 神经网络的层数直接决定了它对现实的刻画能力

这里的“深度”并没有固定的定义 -- 在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。

那么到底几层合适? 每一层是在干什么? 

玄学, 输出全靠猜

参看:
- [神经网络的每一层网络(针对特定的问题)有什么实际的意义吗？](https://www.zhihu.com/question/47732823)
- [卷积神经网络的卷积核大小、卷积层数、每层map个数都是如何确定下来的呢？](https://www.zhihu.com/question/38098038)
